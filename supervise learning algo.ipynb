{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "from sklearn import datasets, linear_model, metrics \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')  \n",
    "X = df[['x1','x2']]\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegression():\n",
    "    \n",
    "    # splitting X and y into training and testing sets \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
    "\n",
    "    # create linear regression object \n",
    "    reg = linear_model.LinearRegression() \n",
    "\n",
    "    # train the model using the training sets \n",
    "    reg.fit(X_train, y_train) \n",
    "\n",
    "    # regression coefficients \n",
    "    print('Coefficients: \\n', reg.coef_) \n",
    "\n",
    "    # variance score: 1 means perfect prediction \n",
    "    print('accuracy score: {}'.format(reg.score(X_test, y_test))) \n",
    "\n",
    "    '''# plot for residual error \n",
    "\n",
    "    ## setting plot style \n",
    "    plt.style.use('fivethirtyeight') \n",
    "\n",
    "    ## plotting residual errors in training data \n",
    "    plt.scatter(reg.predict(X_train), reg.predict(X_train) - y_train, color = \"green\", s = 10, label = 'Train data') \n",
    "\n",
    "    ## plotting residual errors in test data \n",
    "    plt.scatter(reg.predict(X_test), reg.predict(X_test) - y_test, color = \"blue\", s = 10, label = 'Test data') \n",
    "\n",
    "    ## plotting line for zero residual error \n",
    "    plt.hlines(y = 0, xmin = 0, xmax = 50, linewidth = 2) \n",
    "\n",
    "    ## plotting legend \n",
    "    plt.legend(loc = 'upper right') \n",
    "\n",
    "    ## plot title \n",
    "    plt.title(\"Residual errors\") \n",
    "\n",
    "    ## function to show plot \n",
    "    plt.show() '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegerssion():\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    X.max().max()\n",
    "    \n",
    "    clf_lr = LogisticRegression()\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf_lr.predict(X_test)\n",
    "\n",
    "    acc_lr = accuracy_score(y_pred, y_test)\n",
    "    \n",
    "    print(acc_lr)\n",
    "    \n",
    "    '''# plotting logistic regression decision boundary\n",
    "    xx, yy = np.mgrid[X.min().min():X.max().max():0.01, X.min().min():X.max().max():0.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    probs = clf_lr.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(8, 6))\n",
    "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "\n",
    "    ax_c = f.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "    ax.scatter(X['x1'], X['x2'], c=y, s=50,\n",
    "               cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "               edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "    ax.set(aspect=\"equal\",\n",
    "           xlim=(X.min().min(), X.max().max()), ylim=(y.min(), y.max()),\n",
    "           xlabel=\"$X_1$\", ylabel=\"$X_2$\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree():\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    X.max().max()\n",
    "    \n",
    "    clf_dt = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "    y_pred_dt = clf_dt.predict(X_test)\n",
    "    acc_dt = accuracy_score(y_pred_dt, y_test)\n",
    "    print(acc_dt)\n",
    "    \n",
    "    '''xx, yy = np.mgrid[X.min().min():X.max().max():0.01, X.min().min():X.max().max():0.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    probs = clf_dt.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(8, 6))\n",
    "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "\n",
    "    ax_c = f.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "    ax.scatter(X['x1'], X['x2'], c=y, s=50,\n",
    "               cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "               edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "    ax.set(aspect=\"equal\",\n",
    "           xlim=(X.min().min(), X.max().max()), ylim=(y.min(), y.max()),\n",
    "           xlabel=\"$X_1$\", ylabel=\"$X_2$\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm():\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    X.max().max()\n",
    "    \n",
    "    clf_svm = SVC(gamma='auto').fit(X_train, y_train)\n",
    "    y_pred_svm = clf_svm.predict(X_test)\n",
    "    acc_svm = accuracy_score(y_pred_svm, y_test)\n",
    "    print(acc_svm)\n",
    "    \n",
    "    '''plot_decision_regions(X=X.values, \n",
    "                      y=y.values,\n",
    "                      clf=clf_svm, \n",
    "                      legend=2)\n",
    "\n",
    "    # Update plot object with X/Y axis labels and Figure Title\n",
    "    plt.xlabel(X.columns[0], size=14)\n",
    "    plt.ylabel(X.columns[1], size=14)\n",
    "    plt.title('SVM Decision Region Boundary', size=16)'''\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index():\n",
    "    X_train, X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)\n",
    "    \n",
    "    X_train.shape\n",
    "    X_test.shape\n",
    "    y_train.shape\n",
    "    y_test.shape\n",
    "    \n",
    "    dt_entropy = DecisionTreeClassifier(max_depth = 8,criterion = 'entropy',random_state = 3)\n",
    "    dt_entropy.fit(X_train,y_train)\n",
    "    y_pred = dt_entropy.predict(X_test)\n",
    "    accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    dt_gini = DecisionTreeClassifier(max_depth = 8,criterion = 'entropy',random_state = 3)\n",
    "    dt_gini.fit(X_train,y_train)\n",
    "    y_pred_gini = dt_gini.predict(X_test)\n",
    "    accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "    \n",
    "    print('accuracy achieved by using entropy:',accuracy_entropy)\n",
    "    print('accuracy achieved by using gini:',accuracy_gini)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set rmse of dt:{:.2f} 0.4051578612861701\n",
      "test set rmse of dt:{:.2f} 0.48989794855663565\n",
      "0.31602961432506904\n"
     ]
    }
   ],
   "source": [
    "def decisionTreeRegression():\n",
    "    X_train, X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)\n",
    "    \n",
    "    X_train.shape\n",
    "    X_test.shape\n",
    "    y_train.shape\n",
    "    y_test.shape\n",
    "    \n",
    "    dt = DecisionTreeRegressor(max_depth = 8, min_samples_leaf = 0.13, random_state = 3)\n",
    "    dt.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = dt.predict(X_test)\n",
    "    \n",
    "    rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    sd = np.std(y_test)\n",
    "    \n",
    "    print('test set rmse of dt:{:.2f}',format(rmse_dt))\n",
    "    print('test set rmse of dt:{:.2f}',format(sd))\n",
    "    \n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "    print(r2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('linear regerssion')\n",
    "linearRegression()\n",
    "print('logistic regression')\n",
    "logisticRegerssion()\n",
    "print('decision tree classification')\n",
    "decisionTree()\n",
    "print('support vector machine')\n",
    "svm()\n",
    "print('gini index')\n",
    "gini_index()\n",
    "print('decision tree regerssion')\n",
    "decisionTreeRegression()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
